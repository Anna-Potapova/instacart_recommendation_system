{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skillbox - диплом по рекомендательным системам_NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPbZI23Il7jn"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "import gc                         \r\n",
        "gc.enable()\r\n",
        "import tensorflow as tf\r\n",
        "from keras.regularizers import l1, l2\r\n",
        "from keras.layers import Embedding, Input, Dense, Reshape, Flatten, Dropout, Concatenate, Multiply\r\n",
        "from sklearn.preprocessing import StandardScaler \r\n",
        "scale_features_std = StandardScaler() "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G5vDEBT2xD0"
      },
      "source": [
        "class PREDICTION_NN(tf.keras.Model):\r\n",
        "  \r\n",
        "  def __init__(self):\r\n",
        "        \r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "  def prepare_data(path_to_file, sep_type, diff_days):\r\n",
        "      print('Загружаем данные из файла')\r\n",
        "      df_transactions = pd.read_csv(path_to_file, sep=sep_type)#для работы в Colab df = pd.read_csv(\"'/content/drive/MyDrive/transactions.csv', sep=','\")\r\n",
        "\r\n",
        "      print('Формируем датафрейм с максимальным количеством заказов для каждого пользователя') \r\n",
        "      df_user = df_transactions.groupby('user_id')[['order_number']].max()\r\n",
        "      df_user.columns = ['user_total_orders']\r\n",
        "      df_user = df_user.reset_index()\r\n",
        "\r\n",
        "      print('Формируем вспомогательный датафрейм с деталями заказов')\r\n",
        "      df_orders = df_transactions[['user_id', 'order_id', 'product_id', 'add_to_cart_order', 'reordered']]\r\n",
        "\r\n",
        "      print(\"Вычисляем среднее по столбцу 'reordered' для каждого пользователя и формируем датафрейм\")\r\n",
        "      df_user_reorder = df_transactions.groupby('user_id')['reordered'].mean().to_frame('user_reordered_ratio').reset_index()\r\n",
        "\r\n",
        "      print(\"Вычисляем среднее значение позиции товара в чеке по каждой паре 'пользователь/товар', затем сортируем по возрастанию\")\r\n",
        "      df_add_to_cart_order = df_transactions.groupby(['user_id', 'product_id'])['add_to_cart_order'].mean().to_frame('user_add_to_cart_order_ratio').reset_index()\r\n",
        "      df_add_to_cart_order = df_add_to_cart_order.sort_values(by=['user_id', 'user_add_to_cart_order_ratio'])\r\n",
        "\r\n",
        "      print('Объединяем df_transactions и df_user')\r\n",
        "      df_transactions = pd.merge(df_transactions, df_user, on=['user_id'], how='left')\r\n",
        "      \r\n",
        "      print(\"Добавляем столбец 'diff', представляющий собой разницу между общим кол-вом заказов и текущим номером заказа\")\r\n",
        "      df_transactions['diff'] = df_transactions['user_total_orders'] - df_transactions['order_number']\r\n",
        "\r\n",
        "      print('Объединяем df_user_reorder и df_user')\r\n",
        "      df_user = df_user.merge(df_user_reorder, on='user_id', how='left')\r\n",
        "\r\n",
        "      del df_user_reorder\r\n",
        "      gc.collect()\r\n",
        "\r\n",
        "      print('Находим общее количество приобретенных товаров по каждому product_id\tи формируем датафрейм')\r\n",
        "      df_purchases = df_transactions.groupby('product_id')['order_id'].count().to_frame('product_total_purchases').reset_index()\r\n",
        "\r\n",
        "      print(\"Вычисляем среднее по столбцу 'reordered' для каждого товара и формируем датафрейм\")\r\n",
        "      df_product_reorder = df_transactions.groupby('product_id')['reordered'].mean().to_frame('product_reorder_ratio').reset_index()\r\n",
        "\r\n",
        "      print('Объединяем df_purchases и df_product_reorder')\r\n",
        "      df_purchases = df_purchases.merge(df_product_reorder, on='product_id', how='left')\r\n",
        "\r\n",
        "      del df_product_reorder\r\n",
        "      gc.collect()\r\n",
        "\r\n",
        "      print(\"Удаляем Nan из столбца 'product_reorder_ratio'\")\r\n",
        "      df_purchases['product_reorder_ratio'] = df_purchases['product_reorder_ratio'].fillna(value=0)\r\n",
        "\r\n",
        "      print(\"Находим общее количество товаров по каждой паре 'пользователь/товар', сортируем столбец по убыванию 'user_product_total_purchases'\")\r\n",
        "      df_user_product_purchases = df_transactions.groupby(['user_id', 'product_id'])['order_id'].count().to_frame('user_product_total_purchases').reset_index()\r\n",
        "      df_user_product_purchases = df_user_product_purchases.sort_values(by=['user_id', 'user_product_total_purchases'], ascending = [True, False])\r\n",
        "\r\n",
        "      df_total_orders = df_transactions.groupby('user_id')['order_number'].max().to_frame('total_orders')\r\n",
        "\r\n",
        "      print(\"Формируем датафрейм с номерами заказов, в которых впервые встречается каждая пара 'пользователь/товар'\")\r\n",
        "      df_first_order = df_transactions.groupby(['user_id', 'product_id'])['order_number'].min().to_frame('first_order_number').reset_index()\r\n",
        "\r\n",
        "      print('Формируем датафрейм как объединение df_total_orders и df_first_order')\r\n",
        "      df_span = pd.merge(df_total_orders, df_first_order, on='user_id', how='right')\r\n",
        "\r\n",
        "      print(\"Добавляем столбец 'order_range_denominator', представляющий разность между общим количеством заказов и первым заказом\")\r\n",
        "      print(\"по каждой паре 'пользователь/товар', сортируем 'order_range_denominator' по убыванию\")\r\n",
        "      df_span['order_range_denominator'] = df_span['total_orders'] - df_span['first_order_number'] + 1\r\n",
        "      df_span = df_span.sort_values(by=['user_id', 'order_range_denominator'], ascending = [True, False])\r\n",
        "\r\n",
        "      print('Объединяем df_user_product_purchases и df_span')\r\n",
        "      df_user_product_ratio = pd.merge(df_user_product_purchases, df_span, on=['user_id', 'product_id'], how='left')\r\n",
        "\r\n",
        "      print(\"Добавляем столбец 'user_product_reorder_ratio', представляющий собой отношение\")\r\n",
        "      print(\"общего кол-ва покупок к деноминатору заказа по каждой паре 'пользователь/товар'\")\r\n",
        "      print(\"Сортируем 'user_product_reorder_ratio' по убыванию\")\r\n",
        "      df_user_product_ratio['user_product_reorder_ratio'] = df_user_product_ratio['user_product_total_purchases'] / df_user_product_ratio['order_range_denominator']\r\n",
        "      df_user_product_ratio = df_user_product_ratio.sort_values(by=['user_id', 'user_product_reorder_ratio'], ascending=[True, False])\r\n",
        "\r\n",
        "      del df_user_product_ratio['user_product_total_purchases']\r\n",
        "      del df_user_product_ratio['total_orders']\r\n",
        "      del df_user_product_ratio['first_order_number']\r\n",
        "      del df_user_product_ratio['order_range_denominator']\r\n",
        "      del df_span\r\n",
        "      del df_first_order\r\n",
        "      gc.collect()\r\n",
        "\r\n",
        "      print(\"Объединяем df_user_product_ratio и df_user_product_purchases\")\r\n",
        "      print(\"Сортируем 'user_product_reorder_ratio' по убыванию\")\r\n",
        "      df_user_product_purchases = df_user_product_purchases.merge(df_user_product_ratio, on=['user_id', 'product_id'], how='left')\r\n",
        "      df_user_product_purchases = df_user_product_purchases.sort_values(by=['user_id', 'user_product_reorder_ratio'], ascending=[True, False])\r\n",
        "\r\n",
        "      del df_user_product_ratio\r\n",
        "      gc.collect()\r\n",
        "\r\n",
        "      print(\"Добавляем столбец 'back_order_no', который представляет собой разницу между максимальным значением по столбцу 'order_number'\") \r\n",
        "      print(\"для каждого пользователя и текущим номером заказа\")\r\n",
        "      df_transactions['back_order_no'] = df_transactions.groupby('user_id')['order_number'].transform(max) - df_transactions['order_number'] + 1\r\n",
        "\r\n",
        "      print(\"Формируем датафрейм, состоящий только из последних пяти заказов каждого пользователя\")\r\n",
        "      df_transactions_5 = df_transactions.loc[df_transactions['back_order_no'] <= 5]\r\n",
        "\r\n",
        "      del df_transactions\r\n",
        "      gc.collect()\r\n",
        "\r\n",
        "      print(\"Создаем тренировочный и тестовый датасеты\")\r\n",
        "      print(\"train представляет собой заказы всех пользователей за исключением заказов последних 'diff_days'\")\r\n",
        "      print(\"test - заказы всех пользователей в последние 'diff_days'\")\r\n",
        "      #рекомендованное значение diff_days = 1\r\n",
        "      train = df_transactions_5.loc[df_transactions_5['diff']>diff_days]\r\n",
        "      test = df_transactions_5.loc[df_transactions_5['diff']<=diff_days]\r\n",
        "\r\n",
        "      print(\"Формируем датафрейм с количеством заказов, в которых присутствовали определенные товары в последних пяти заказах\")\r\n",
        "      orders_5_train = train.groupby(['user_id','product_id'])[['order_id']].count()\r\n",
        "      orders_5_train.columns = ['last_5']\r\n",
        "\r\n",
        "      print(\"Объединяем orders_5_train с датафреймом, представляющим только те товары, которые перезаказывались\")\r\n",
        "      orders_6_train = df_transactions_5.groupby(['user_id','product_id'])[['reordered']].max()\r\n",
        "      orders_5_train = orders_5_train.merge(orders_6_train, on=['user_id', 'product_id'], how='left')\r\n",
        "\r\n",
        "      del orders_6_train\r\n",
        "      gc.collect()\r\n",
        "\r\n",
        "      print(\"Объединяем df_user_product_purchases и orders_5_train\")\r\n",
        "      df_user_product_purchases_train = df_user_product_purchases.merge(orders_5_train, on=['user_id', 'product_id'], how='inner')\r\n",
        "\r\n",
        "      print(\"Объединяем df_user_product_purchases_train и df_user\")\r\n",
        "      train = df_user_product_purchases_train.merge(df_user, on='user_id', how='inner')\r\n",
        "\r\n",
        "      print(\"Объединяем train и df_purchases\")\r\n",
        "      train = train.merge(df_purchases, on='product_id', how='inner')\r\n",
        "\r\n",
        "      print(\"Формируем датафрейм с количеством заказов, в которых присутствовали определенные товары в последних пяти заказах\")\r\n",
        "      orders_5_test = test.groupby(['user_id','product_id'])[['order_id']].count()\r\n",
        "      orders_5_test.columns = ['last_5']\r\n",
        "\r\n",
        "      print(\"Объединяем orders_5_test с датафреймом, представляющим только те товары, которые перезаказывались\")\r\n",
        "      orders_6_test = df_transactions_5.groupby(['user_id','product_id'])[['reordered']].max()\r\n",
        "      orders_5_test = orders_5_test.merge(orders_6_test, on=['user_id', 'product_id'], how='left')\r\n",
        "\r\n",
        "      del orders_6_test\r\n",
        "      gc.collect()\r\n",
        "\r\n",
        "      print(\"Объединяем df_user_product_purchases и orders_5_test\")\r\n",
        "      df_user_product_purchases_test = df_user_product_purchases.merge(orders_5_test, on=['user_id', 'product_id'], how='inner')\r\n",
        "\r\n",
        "      print(\"Объединяем df_user_product_purchases_test и df_user\")\r\n",
        "      test = df_user_product_purchases_test.merge(df_user, on='user_id', how='inner')\r\n",
        "      test = test.merge(df_purchases, on='product_id', how='inner')\r\n",
        "\r\n",
        "      del df_user\r\n",
        "      del df_purchases\r\n",
        "      del df_user_product_purchases\r\n",
        "      gc.collect()\r\n",
        "\r\n",
        "      print('Формируем x_train, y_train, x_test, y_test')\r\n",
        "      y_train = train['reordered'].ravel()#target тренировочный\r\n",
        "      x_train = train.drop(['reordered'], axis=1)#признаки без target тренировочные\r\n",
        "      y_test = test['reordered'].ravel()#target тестовый\r\n",
        "      x_test = test.drop(['reordered'], axis=1)#признаки без target тестовые\r\n",
        "\r\n",
        "      return x_train, y_train, x_test, y_test\r\n",
        "  \r\n",
        "  def preprocess_data(x_train, x_test, scale_features_std):\r\n",
        "    #scale_features_std = StandardScaler()\r\n",
        "    user_train = x_train[['user_id', 'user_total_orders', 'user_reordered_ratio']]\r\n",
        "    scaler = scale_features_std.fit(user_train)\r\n",
        "    user_train = scale_features_std.transform(user_train)\r\n",
        "\r\n",
        "    product_train = x_train[['product_id', 'user_product_total_purchases', 'user_product_reorder_ratio', 'last_5', 'product_total_purchases', 'product_reorder_ratio']]\r\n",
        "    scaler = scale_features_std.fit(product_train)\r\n",
        "    product_train = scale_features_std.transform(product_train)\r\n",
        "    \r\n",
        "    user_test = x_test[['user_id', 'user_total_orders', 'user_reordered_ratio']]\r\n",
        "    scaler = scale_features_std.fit(user_test)\r\n",
        "    user_test = scale_features_std.transform(user_test)\r\n",
        "    \r\n",
        "    product_test = x_test[['product_id', 'user_product_total_purchases', 'user_product_reorder_ratio', 'last_5', 'product_total_purchases', 'product_reorder_ratio']]\r\n",
        "    scaler = scale_features_std.fit(product_test)\r\n",
        "    product_test = scale_features_std.transform(product_test)\r\n",
        "    return user_train, product_train, user_test, product_test\r\n",
        "  \r\n",
        "  def create_model(layers, reg_layers, user_n_factors, product_n_factors):\r\n",
        "    #например, layers = [20, 10]\r\n",
        "    #например, reg_layers=[0, 0]\r\n",
        "    assert len(layers) == len(reg_layers)\r\n",
        "    num_layer = len(layers)\r\n",
        "      \r\n",
        "    user_input = tf.keras.Input(shape=(user_n_factors,), dtype='int64', name='user')\r\n",
        "    product_input = tf.keras.Input(shape=(product_n_factors,), dtype='int64', name='item')\r\n",
        "      \r\n",
        "    user_latent = tf.keras.layers.Flatten()(user_input)\r\n",
        "    product_latent = tf.keras.layers.Flatten()(product_input)\r\n",
        "      \r\n",
        "    vector = Concatenate()([user_latent, product_latent])\r\n",
        "      \r\n",
        "    for idx in range(1, num_layer):\r\n",
        "      layer = Dense(layers[idx], kernel_regularizer = l2(reg_layers[idx]), activation='relu', name = 'layer%d' %idx)\r\n",
        "      vector = layer(vector)\r\n",
        "      \r\n",
        "    prediction = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(vector)\r\n",
        "      \r\n",
        "    model = tf.keras.models.Model(inputs = [user_input, product_input], outputs = prediction)\r\n",
        "      \r\n",
        "    return model\r\n",
        "    \r\n",
        "  def compile_model(model, optimizer, loss):\r\n",
        "    #optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n",
        "    loss = tf.keras.losses.MSE\r\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['mse'])\r\n",
        "    model.summary()\r\n",
        "\r\n",
        "    #функция предназначена для создания датафрейма из тестового набора данных и списка полученных предсказаний\r\n",
        "  def create_predicted_df(users, products, predictions, columns=['user_id', 'product_id', 'predictions']):\r\n",
        "    predicted_df = pd.DataFrame(list(zip(users, products, predictions)), columns=columns)\r\n",
        "    def extract_value(array):\r\n",
        "      return array[0]\r\n",
        "    predicted_df['predictions'] = predicted_df['predictions'].apply(extract_value)\r\n",
        "    predicted_df = predicted_df.sort_values(by=['user_id', 'predictions'], ascending=[True, False])\r\n",
        "    return predicted_df\r\n",
        "\r\n",
        "  def clean_prediction(row):\r\n",
        "    data = row.product_id\r\n",
        "    data = str(\"\".join(str(data))[1:-1].replace(',', ' '))\r\n",
        "    return data    \r\n",
        "  \r\n",
        "  def get_recommendations(predicted_df):\r\n",
        "    rec = predicted_df.groupby('user_id')['product_id'].apply(list).reset_index(name='product_id')\r\n",
        "    #функция предназначена для удаления нулевых элементов из списка,\r\n",
        "    #а также для формирования срезов из 10-ти элементов      \r\n",
        "    def fetch_ten(l):\r\n",
        "      for i in l:\r\n",
        "        if i == 0:\r\n",
        "          l.remove(i)\r\n",
        "        elif len(l)>10:\r\n",
        "          return l[:10]\r\n",
        "        else:\r\n",
        "          return l\r\n",
        "    rec['product_id'] = rec['product_id'].apply(fetch_ten)\r\n",
        "    #функция предназначена для приведения строк к формату, который требуется в условии задачи\r\n",
        "    def clean_prediction(row):\r\n",
        "      data = row.product_id\r\n",
        "      data = str(\"\".join(str(data))[1:-1].replace(',', ' '))\r\n",
        "      return data\r\n",
        "    rec['product_id'] = rec.apply(clean_prediction, axis=1)\r\n",
        "    return rec\r\n",
        "    \r\n",
        "  def get_recommendations_for_user(path_to_submission_file, path_to_products_file, user_id):\r\n",
        "    final_rec = pd.read_csv(path_to_submission_file)\r\n",
        "    products = pd.read_csv(path_to_products_file)\r\n",
        "    rec_for_user = final_rec.loc[final_rec['user_id'] == user_id]\r\n",
        "    s = (rec_for_user['product_id'].to_list())[0].split()\r\n",
        "    for i in s:\r\n",
        "      for index, row in products.iterrows():\r\n",
        "        if row['product_id'] == int(i):\r\n",
        "          print(row['product_name'])"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VmhvZy9sgjw",
        "outputId": "dd64e540-9f30-4928-8af3-9448c4196345"
      },
      "source": [
        "x_train_df, y_train_df, x_test_df, y_test_df = PREDICTION_NN.prepare_data('/content/drive/MyDrive/transactions.csv', ',', 1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Загружаем данные из файла\n",
            "Формируем датафрейм с максимальным количеством заказов для каждого пользователя\n",
            "Формируем вспомогательный датафрейм с деталями заказов\n",
            "Вычисляем среднее по столбцу 'reordered' для каждого пользователя и формируем датафрейм\n",
            "Вычисляем среднее значение позиции товара в чеке по каждой паре 'пользователь/товар', затем сортируем по возрастанию\n",
            "Объединяем df_transactions и df_user\n",
            "Добавляем столбец 'diff', представляющий собой разницу между общим кол-вом заказов и текущим номером заказа\n",
            "Объединяем df_user_reorder и df_user\n",
            "Находим общее количество приобретенных товаров по каждому product_id\tи формируем датафрейм\n",
            "Вычисляем среднее по столбцу 'reordered' для каждого товара и формируем датафрейм\n",
            "Объединяем df_purchases и df_product_reorder\n",
            "Удаляем Nan из столбца 'product_reorder_ratio'\n",
            "Находим общее количество товаров по каждой паре 'пользователь/товар', сортируем столбец по убыванию 'user_product_total_purchases'\n",
            "Формируем датафрейм с номерами заказов, в которых впервые встречается каждая пара 'пользователь/товар'\n",
            "Формируем датафрейм как объединение df_total_orders и df_first_order\n",
            "Добавляем столбец 'order_range_denominator', представляющий разность между общим количеством заказов и первым заказом\n",
            "по каждой паре 'пользователь/товар', сортируем 'order_range_denominator' по убыванию\n",
            "Объединяем df_user_product_purchases и df_span\n",
            "Добавляем столбец 'user_product_reorder_ratio', представляющий собой отношение\n",
            "общего кол-ва покупок к деноминатору заказа по каждой паре 'пользователь/товар'\n",
            "Сортируем 'user_product_reorder_ratio' по убыванию\n",
            "Объединяем df_user_product_ratio и df_user_product_purchases\n",
            "Сортируем 'user_product_reorder_ratio' по убыванию\n",
            "Добавляем столбец 'back_order_no', который представляет собой разницу между максимальным значением по столбцу 'order_number'\n",
            "для каждого пользователя и текущим номером заказа\n",
            "Формируем датафрейм, состоящий только из последних пяти заказов каждого пользователя\n",
            "Создаем тренировочный и тестовый датасеты\n",
            "train представляет собой заказы всех пользователей за исключением заказов последних 'diff_days'\n",
            "test - заказы всех пользователей в последние 'diff_days'\n",
            "Формируем датафрейм с количеством заказов, в которых присутствовали определенные товары в последних пяти заказах\n",
            "Объединяем orders_5_train с датафреймом, представляющим только те товары, которые перезаказывались\n",
            "Объединяем df_user_product_purchases и orders_5_train\n",
            "Объединяем df_user_product_purchases_train и df_user\n",
            "Объединяем train и df_purchases\n",
            "Формируем датафрейм с количеством заказов, в которых присутствовали определенные товары в последних пяти заказах\n",
            "Объединяем orders_5_test с датафреймом, представляющим только те товары, которые перезаказывались\n",
            "Объединяем df_user_product_purchases и orders_5_test\n",
            "Объединяем df_user_product_purchases_test и df_user\n",
            "Формируем x_train, y_train, x_test, y_test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfEj1qfAyo7f"
      },
      "source": [
        "user_train_df, product_train_df, user_test_df, product_test_df = PREDICTION_NN.preprocess_data(x_train_df, x_test_df, StandardScaler())"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5zRDZZUCDyy"
      },
      "source": [
        "rec_model = PREDICTION_NN.create_model([20, 10], [0, 0], 3, 6)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKkm2aJSCwFX",
        "outputId": "deec6318-6024-4ffa-b14f-c3cc219bcd23"
      },
      "source": [
        "PREDICTION_NN.compile_model(rec_model, tf.keras.optimizers.Adam(learning_rate=1e-3), tf.keras.losses.MSE)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "user (InputLayer)               [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "item (InputLayer)               [(None, 6)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 3)            0           user[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 6)            0           item[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 9)            0           flatten[0][0]                    \n",
            "                                                                 flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer1 (Dense)                  (None, 10)           100         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "prediction (Dense)              (None, 1)            11          layer1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 111\n",
            "Trainable params: 111\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmr3wKH8uT7C",
        "outputId": "f8342429-8533-4791-939c-5b36bb97d737"
      },
      "source": [
        "hist = rec_model.fit([user_train_df, product_train_df], #input\r\n",
        "                 y_train_df, # labels\r\n",
        "                 batch_size=16, epochs=50, steps_per_epoch=10, shuffle=True, validation_split=0.3)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.2261 - mse: 0.2261 - val_loss: 0.2259 - val_mse: 0.2259\n",
            "Epoch 2/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.2207 - mse: 0.2207 - val_loss: 0.2221 - val_mse: 0.2221\n",
            "Epoch 3/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.2164 - mse: 0.2164 - val_loss: 0.2187 - val_mse: 0.2187\n",
            "Epoch 4/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.2159 - mse: 0.2159 - val_loss: 0.2158 - val_mse: 0.2158\n",
            "Epoch 5/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.2139 - mse: 0.2139 - val_loss: 0.2134 - val_mse: 0.2134\n",
            "Epoch 6/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.1963 - mse: 0.1963 - val_loss: 0.2111 - val_mse: 0.2111\n",
            "Epoch 7/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.2104 - mse: 0.2104 - val_loss: 0.2089 - val_mse: 0.2089\n",
            "Epoch 8/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.2031 - mse: 0.2031 - val_loss: 0.2068 - val_mse: 0.2068\n",
            "Epoch 9/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1959 - mse: 0.1959 - val_loss: 0.2050 - val_mse: 0.2050\n",
            "Epoch 10/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1983 - mse: 0.1983 - val_loss: 0.2029 - val_mse: 0.2029\n",
            "Epoch 11/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1815 - mse: 0.1815 - val_loss: 0.2010 - val_mse: 0.2010\n",
            "Epoch 12/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1863 - mse: 0.1863 - val_loss: 0.1994 - val_mse: 0.1994\n",
            "Epoch 13/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.1808 - mse: 0.1808 - val_loss: 0.1980 - val_mse: 0.1980\n",
            "Epoch 14/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1647 - mse: 0.1647 - val_loss: 0.1965 - val_mse: 0.1965\n",
            "Epoch 15/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1726 - mse: 0.1726 - val_loss: 0.1950 - val_mse: 0.1950\n",
            "Epoch 16/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1670 - mse: 0.1670 - val_loss: 0.1938 - val_mse: 0.1938\n",
            "Epoch 17/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1936 - mse: 0.1936 - val_loss: 0.1927 - val_mse: 0.1927\n",
            "Epoch 18/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.1815 - mse: 0.1815 - val_loss: 0.1917 - val_mse: 0.1917\n",
            "Epoch 19/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1588 - mse: 0.1588 - val_loss: 0.1908 - val_mse: 0.1908\n",
            "Epoch 20/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.1581 - mse: 0.1581 - val_loss: 0.1899 - val_mse: 0.1899\n",
            "Epoch 21/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1449 - mse: 0.1449 - val_loss: 0.1890 - val_mse: 0.1890\n",
            "Epoch 22/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.1727 - mse: 0.1727 - val_loss: 0.1880 - val_mse: 0.1880\n",
            "Epoch 23/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1867 - mse: 0.1867 - val_loss: 0.1871 - val_mse: 0.1871\n",
            "Epoch 24/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1858 - mse: 0.1858 - val_loss: 0.1862 - val_mse: 0.1862\n",
            "Epoch 25/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1656 - mse: 0.1656 - val_loss: 0.1856 - val_mse: 0.1856\n",
            "Epoch 26/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1414 - mse: 0.1414 - val_loss: 0.1850 - val_mse: 0.1850\n",
            "Epoch 27/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.1605 - mse: 0.1605 - val_loss: 0.1843 - val_mse: 0.1843\n",
            "Epoch 28/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.1377 - mse: 0.1377 - val_loss: 0.1835 - val_mse: 0.1835\n",
            "Epoch 29/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1635 - mse: 0.1635 - val_loss: 0.1828 - val_mse: 0.1828\n",
            "Epoch 30/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1576 - mse: 0.1576 - val_loss: 0.1820 - val_mse: 0.1820\n",
            "Epoch 31/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1756 - mse: 0.1756 - val_loss: 0.1813 - val_mse: 0.1813\n",
            "Epoch 32/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.1531 - mse: 0.1531 - val_loss: 0.1810 - val_mse: 0.1810\n",
            "Epoch 33/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1687 - mse: 0.1687 - val_loss: 0.1809 - val_mse: 0.1809\n",
            "Epoch 34/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1593 - mse: 0.1593 - val_loss: 0.1805 - val_mse: 0.1805\n",
            "Epoch 35/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.1536 - mse: 0.1536 - val_loss: 0.1800 - val_mse: 0.1800\n",
            "Epoch 36/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1402 - mse: 0.1402 - val_loss: 0.1795 - val_mse: 0.1795\n",
            "Epoch 37/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.1587 - mse: 0.1587 - val_loss: 0.1791 - val_mse: 0.1791\n",
            "Epoch 38/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1458 - mse: 0.1458 - val_loss: 0.1787 - val_mse: 0.1787\n",
            "Epoch 39/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1453 - mse: 0.1453 - val_loss: 0.1781 - val_mse: 0.1781\n",
            "Epoch 40/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1495 - mse: 0.1495 - val_loss: 0.1776 - val_mse: 0.1776\n",
            "Epoch 41/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.1191 - mse: 0.1191 - val_loss: 0.1773 - val_mse: 0.1773\n",
            "Epoch 42/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1713 - mse: 0.1713 - val_loss: 0.1769 - val_mse: 0.1769\n",
            "Epoch 43/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1606 - mse: 0.1606 - val_loss: 0.1764 - val_mse: 0.1764\n",
            "Epoch 44/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1618 - mse: 0.1618 - val_loss: 0.1760 - val_mse: 0.1760\n",
            "Epoch 45/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1609 - mse: 0.1609 - val_loss: 0.1757 - val_mse: 0.1757\n",
            "Epoch 46/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.1526 - mse: 0.1526 - val_loss: 0.1751 - val_mse: 0.1751\n",
            "Epoch 47/50\n",
            "10/10 [==============================] - 33s 4s/step - loss: 0.1361 - mse: 0.1361 - val_loss: 0.1746 - val_mse: 0.1746\n",
            "Epoch 48/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1333 - mse: 0.1333 - val_loss: 0.1740 - val_mse: 0.1740\n",
            "Epoch 49/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1462 - mse: 0.1462 - val_loss: 0.1737 - val_mse: 0.1737\n",
            "Epoch 50/50\n",
            "10/10 [==============================] - 32s 4s/step - loss: 0.1823 - mse: 0.1823 - val_loss: 0.1732 - val_mse: 0.1732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDK64I71z1DY"
      },
      "source": [
        "preds = rec_model.predict([user_test_df, product_test_df])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCJrlc-MMDpX"
      },
      "source": [
        "rec_df = PREDICTION_NN.create_predicted_df(x_test_df['user_id'], x_test_df['product_id'], preds, columns=['user_id', 'product_id', 'predictions'])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "ZGpaGETnOKdz",
        "outputId": "89f7fad4-3685-4b85-e927-d5a02a163d61"
      },
      "source": [
        "rec_df"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2519</th>\n",
              "      <td>1</td>\n",
              "      <td>46149</td>\n",
              "      <td>0.976993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>196</td>\n",
              "      <td>0.964896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1714</th>\n",
              "      <td>1</td>\n",
              "      <td>12427</td>\n",
              "      <td>0.953206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2172</th>\n",
              "      <td>1</td>\n",
              "      <td>25133</td>\n",
              "      <td>0.953206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021</th>\n",
              "      <td>1</td>\n",
              "      <td>10258</td>\n",
              "      <td>0.942491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1386737</th>\n",
              "      <td>206209</td>\n",
              "      <td>6846</td>\n",
              "      <td>0.593171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1548399</th>\n",
              "      <td>206209</td>\n",
              "      <td>5622</td>\n",
              "      <td>0.593171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120183</th>\n",
              "      <td>206209</td>\n",
              "      <td>14197</td>\n",
              "      <td>0.516883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1294603</th>\n",
              "      <td>206209</td>\n",
              "      <td>20590</td>\n",
              "      <td>0.434173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1309543</th>\n",
              "      <td>206209</td>\n",
              "      <td>14727</td>\n",
              "      <td>0.322123</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1839782 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         user_id  product_id  predictions\n",
              "2519           1       46149     0.976993\n",
              "0              1         196     0.964896\n",
              "1714           1       12427     0.953206\n",
              "2172           1       25133     0.953206\n",
              "2021           1       10258     0.942491\n",
              "...          ...         ...          ...\n",
              "1386737   206209        6846     0.593171\n",
              "1548399   206209        5622     0.593171\n",
              "120183    206209       14197     0.516883\n",
              "1294603   206209       20590     0.434173\n",
              "1309543   206209       14727     0.322123\n",
              "\n",
              "[1839782 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wETcrwa7OUrw"
      },
      "source": [
        "final_rec = PREDICTION_NN.get_recommendations(rec_df)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "iLrmSwbCQJHV",
        "outputId": "3942b2ca-c2c3-4d19-c3bd-eaee43be313b"
      },
      "source": [
        "final_rec"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>product_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>46149  196  12427  25133  10258  35951  38928 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>24852  47209  7963  21709  33754  19057  20785...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>47766  39190  17668  21903  43961  1005  32402...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "      <td>21137  47272  40852  37602  37999  27690  2999...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13</td>\n",
              "      <td>27086  4210  43086  27435  42248  41351  5025 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>206202</td>\n",
              "      <td>12919  38837  24852  17038  17459  41177  432 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>206206</td>\n",
              "      <td>27086  11520  29326  13045  45681  38739  1689...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>206207</td>\n",
              "      <td>13176  33754  36011  39180  44632  33787  2734...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>206208</td>\n",
              "      <td>13176  34213  21137  47626  27845  23579  2099...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>206209</td>\n",
              "      <td>24852  16168  9405  19348  6567  39216  31477 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       user_id                                         product_id\n",
              "0            1  46149  196  12427  25133  10258  35951  38928 ...\n",
              "1            2  24852  47209  7963  21709  33754  19057  20785...\n",
              "2            3  47766  39190  17668  21903  43961  1005  32402...\n",
              "3            7  21137  47272  40852  37602  37999  27690  2999...\n",
              "4           13  27086  4210  43086  27435  42248  41351  5025 ...\n",
              "...        ...                                                ...\n",
              "99995   206202  12919  38837  24852  17038  17459  41177  432 ...\n",
              "99996   206206  27086  11520  29326  13045  45681  38739  1689...\n",
              "99997   206207  13176  33754  36011  39180  44632  33787  2734...\n",
              "99998   206208  13176  34213  21137  47626  27845  23579  2099...\n",
              "99999   206209  24852  16168  9405  19348  6567  39216  31477 ...\n",
              "\n",
              "[100000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWzkKKHTKsRV"
      },
      "source": [
        "#формируем файл с рекомендациями\r\n",
        "submission_9 = final_rec.to_csv('submission_9.csv', index=False)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJfpiA6XKyEY",
        "outputId": "9eaa8c41-344e-4365-ee79-0c948c023e0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "preds_for_13 = PREDICTION_NN.get_recommendations_for_user('/content/drive/MyDrive/submission_9.csv', '/content/drive/MyDrive/products.csv', 13)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Half & Half\n",
            "Whole Milk\n",
            "Super Greens Salad\n",
            "Whole Wheat Pita Bread Loaves\n",
            "Coconut Milk, Classic\n",
            "Family Size Naturally Flavored Whole Grain Oats Cereal\n",
            "Green Onions\n",
            "Bok Choy\n",
            "Cilantro Bunch\n",
            "Tomato Paste\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}